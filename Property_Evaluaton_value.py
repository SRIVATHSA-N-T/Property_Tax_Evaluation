# -*- coding: utf-8 -*-
"""SNT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tRXgetj6ktganuTLIFaOSUKKf9UCQmV_

# Importing Necessory Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# data processing
import pandas as pd
# linear algebra
import numpy as np
# data visualization
import matplotlib.pyplot as plt
import seaborn as sns
import klib
import missingno
import scikitplot as skplt
import re


#scaling
from sklearn.preprocessing import StandardScaler

#model selection
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold


#model evaluation
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error

#Importing algorithms
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.ensemble import ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.linear_model import Ridge,Lasso,RidgeCV,LassoCV


import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

"""## Load the data"""

#lets load train and test data sets
train = pd.read_excel("Train-1665386529049.xlsx")
test = pd.read_excel("Test-1665386517573.xlsx")

#cheking our train data
train.head(10)

#cheking our test data
test.head()

#lets check the shape
print("Shape of the train data: ",train.shape)
print("Shape of the test data: ",test.shape)

df  = train
print("Number of rows: ",df.shape[0])
print("Number of columns: ",df.shape[1])

#checking info
df.info()

"""### Checing Missing values"""

df.isna().sum()

klib.missingval_plot(df)

"""Great, there is no any missing value present in our train dataset."""

#Zip code should be a categorical feature, so I will change its data type
df['ZipCode'] = df['ZipCode'].astype('object')
test['ZipCode'] = test['ZipCode'].astype('object')

df.dtypes

#lets plot a cat_plot using klib
klib.dist_plot(df)

#Uique values
{column:len(df[column].unique())for column in df.select_dtypes(include='object')}

#lets check the Uique values  test data set
{column:len(test[column].unique())for column in test.select_dtypes(include='object')}

"""* By oserving the unique values from the features I can say the column __PropertyID__ is having all different unique entries so we can drop this column.

* Also column __State__ is having single entry throughout so I will drop that column as well.
"""

#dropping both columns from train data
df.drop(columns=['State','PropertyID'], inplace = True)

#dropping State column from test data
test.drop(columns=['State'], inplace = True)

#checkig unique etries from contiuous columns
{column:len(df[column].unique())for column in df.select_dtypes(include='int64')}

col=['Borough','NoOfCommercialUnits','TaxClass_AtEvaluationTime']

plt.style.use('default')
plt.figure(figsize=(8,12))
for i in range(len(col)):
    plt.subplot(3,1,i+1)
    sns.stripplot(y=df['PropertyEvaluationvalue'],x=df[col[i]])
    plt.title(f"SalePrice VS {col[i]}",fontsize=15)
    plt.xticks(fontsize=10)  
    plt.yticks(fontsize=10)
    plt.tight_layout()

plt.style.use('default')
plt.figure(figsize=(8,12))
for i in range(len(col)):
    plt.subplot(3,1,i+1)
    sns.barplot(y=df['PropertyEvaluationvalue'],x=df[col[i]])
    plt.title(f"SalePrice VS {col[i]}",fontsize=15)
    plt.xticks(fontsize=10)  
    plt.yticks(fontsize=10)
    plt.tight_layout()

#Lets have a look on distribution of our data
num_data = df._get_numeric_data()
plt.style.use('default')
plt.figure(figsize = (20,25))
plotnumber = 1
for column in num_data:
    if plotnumber <=12:
        ax = plt.subplot(4,3,plotnumber)
        sns.scatterplot(num_data[column],y=df['PropertyEvaluationvalue'])
        plt.title(f"Distribution of {column}",fontsize=20)
        plt.xlabel(column,fontsize = 20)
    plotnumber+=1
plt.tight_layout()

df1=df[df['NoOfResidentialUnits']<150]
df1=df1[df1['NoOfCommercialUnits']<51]
df1=df1[df1['TotalNoOfUnits']<140]

df1.shape

num_data = df1._get_numeric_data()
plt.style.use('default')
plt.figure(figsize = (20,25))
plotnumber = 1
for column in num_data:
    if plotnumber <=12:
        ax = plt.subplot(4,3,plotnumber)
        sns.scatterplot(num_data[column],y=df1['PropertyEvaluationvalue'])
        plt.title(f"Distribution of {column}",fontsize=20)
        plt.xlabel(column,fontsize = 20)
    plotnumber+=1
plt.tight_layout()

"""Looking at these strip plots we don't find any trend, but there may some outliers present in __NoOfCommercialUnits__."""

#Lets have a look on distribution of our data
num_data = df1._get_numeric_data()
plt.style.use('default')
plt.figure(figsize = (20,25))
plotnumber = 1
for column in num_data:
    if plotnumber <=12:
        ax = plt.subplot(4,3,plotnumber)
        sns.distplot(num_data[column],hist=False, color="red", kde_kws={"shade": True})
        plt.title(f"Distribution of {column}",fontsize=20)
        plt.xlabel(column,fontsize = 20)
    plotnumber+=1
plt.tight_layout()

"""### Creating new column using Address """

#Noice removal
def num_rem(text):
    #remove non-ascii and digits
    text = re.sub("(\\d)", "", text)
    #remove white space
    text = text.strip()
    return text

df=df1

df['Add_new'] = df['Address'].apply(lambda x : num_rem(x))

df['Add_new'].nunique()

df['Add_new'].head(10)

"""## Creating a new column using DateOfEvaluation &YearOfConstruction"""

df["Eval_year"] = df["DateOfEvaluation"].astype(str).apply(lambda x: x[0:4])

df["Eval_year"]=df["Eval_year"].astype('int64')

df['House_age'] = df["Eval_year"]-df['YearOfConstruction']
df['House_age'].head()

df['House_age'].nunique()

df["Eval_year"]=df["Eval_year"].astype('object')

df.describe().T

#Lets have a look on distribution of our data
cols=['Block','Lot','NoOfResidentialUnits','NoOfCommercialUnits','TotalNoOfUnits','LandAreaInSqFt','GrossAreaInSqFt','House_age']
plt.style.use('default')
plt.figure(figsize = (20,25))
plotnumber = 1
for column in cols:
    if plotnumber <=12:
        ax = plt.subplot(4,3,plotnumber)
        sns.boxenplot(df[column], color="red")
        #plt.title(f"Distribution of {column}",fontsize=20)
        plt.xlabel(column,fontsize = 20)
    plotnumber+=1
plt.tight_layout()

upper_limit = df.GrossAreaInSqFt.mean() + 3*df.GrossAreaInSqFt.std()
lower_limit = df.GrossAreaInSqFt.mean() -3*df.GrossAreaInSqFt.std()
print(upper_limit)
print(lower_limit)

df[(df.GrossAreaInSqFt>upper_limit) | (df.GrossAreaInSqFt<lower_limit)]

"""## Now remove these outliers and generate new dataframe"""

test.GrossAreaInSqFt.max()

df_new = df[(df.GrossAreaInSqFt<upper_limit) & (df.GrossAreaInSqFt>lower_limit)]
df_new.head()

df_new.shape

sns.boxplot(df_new['GrossAreaInSqFt'])

max_thresold = df_new['GrossAreaInSqFt'].quantile(0.98)
min_thresold = df_new['GrossAreaInSqFt'].quantile(0.05)
print(max_thresold)
print(min_thresold)

df[(df.GrossAreaInSqFt>max_thresold) | (df.GrossAreaInSqFt<min_thresold)]

df_new1=df[(df['GrossAreaInSqFt']<max_thresold) & (df['GrossAreaInSqFt']>min_thresold)]

df_new1.shape

print("Percetage of data removed outliers:",(df.shape[0]-df_new1.shape[0])/df.shape[0]*100)

sns.boxplot(df_new1['GrossAreaInSqFt'])

#Lets have a look on distribution of our data
cols=['Block','Lot','NoOfResidentialUnits','NoOfCommercialUnits','TotalNoOfUnits','LandAreaInSqFt','GrossAreaInSqFt','House_age']
plt.style.use('default')
plt.figure(figsize = (20,25))
plotnumber = 1
for column in cols:
    if plotnumber <=12:
        ax = plt.subplot(4,3,plotnumber)
        sns.boxenplot(df_new1[column], color="red")
        #plt.title(f"Distribution of {column}",fontsize=20)
        plt.xlabel(column,fontsize = 20)
    plotnumber+=1
plt.tight_layout()

klib.corr_mat(df_new1)

klib.corr_plot(df_new1)

df_new1.drop(columns=['LandAreaInSqFt','NoOfCommercialUnits','TotalNoOfUnits','NoOfResidentialUnits'], inplace = True)

"""As I don't see any strong correlation between target and LandAreaInSqFt, I will drop this column"""

#dropping LandAreaInSqFt 
#df_new1.drop(columns=['LandAreaInSqFt'], inplace = True)

df_new1= pd.get_dummies(df_new1, columns=['Borough','TaxClass_AtEvaluationTime'],drop_first=True)

df_new1.head(5)

df_new1.info()

{column:len(test[column].unique())for column in test.select_dtypes(include='object')}

"""## Split Data into x & y"""

#lets saperate data into label and features
x = df_new1.drop(columns = ['PropertyEvaluationvalue','Address','DateOfEvaluation'])
y = df_new1["PropertyEvaluationvalue"]

x.skew()

"""Lot,GrossAreaInSqFt                             """

#lets check the count of airlines and check the relation between airline and Price
sns.set_theme()
plt.figure(figsize = (20,8))
plt.subplot(1,2,1)
sns.distplot(x.Lot)
plt.title('Distriution for Lot')

plt.subplot(1,2,2)
sns.distplot(x.GrossAreaInSqFt)
plt.title('Distriution for GrossAreaInSqFt')

plt.show()

"""#### To treat the skewness, I will apply log trasformation to these two columns"""

x['Lot'] = np.log(x['Lot'])
x['GrossAreaInSqFt'] = np.log(x['GrossAreaInSqFt'])

sns.set_theme()
plt.figure(figsize = (20,8))
plt.subplot(1,2,1)
sns.distplot(x.Lot)
plt.title('Distriution for Lot')

plt.subplot(1,2,2)
sns.distplot(x.GrossAreaInSqFt)
plt.title('Distriution for GrossAreaInSqFt')

plt.show()

"""great, we have got better distriution"""

cat_data  = x.select_dtypes(include = 'object')
num_data = x.select_dtypes(include = [np.number])
from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
for i in cat_data.columns:
    cat_data[i] = enc.fit_transform(cat_data[i].values.reshape(-1,1))

num_data

num = num_data.reset_index(drop=True)
cat_data = cat_data.reset_index(drop=True)

num.head()

"""## Combining categorical and numerical data"""

X = pd.concat([num, cat_data], axis = 1)

X.head()

X.shape

"""## Applying standard scaler to numerical data"""

#Lets bring all numerical features to common scale by applying standard scaler

scaler = StandardScaler()
X_std = scaler.fit_transform(X)
X_std = pd.DataFrame(X_std,columns=X.columns)

X_std.head()

X_std.shape

"""## Applying log transformation to our target variable"""

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
plt.title("Normal Distribution",fontsize=15)
sns.distplot(y)
plt.subplot(1,2,2)
plt.title("Distribution after log transformation",fontsize=15)
sns.distplot(np.log(y))
plt.show()

#lets split our train data into train and test part with our best random state
x_train, x_test, y_train, y_test = train_test_split(X_std,y,test_size = 0.25,random_state = 1)

x_train.shape

models={
    "LinearRegression":LinearRegression(),
    "DecisionTreeRegressor":DecisionTreeRegressor(),
    "RandomForestRegressor":RandomForestRegressor(),
    "XGBRegressor":XGBRegressor(),
    "ExtraTreesRegressor":ExtraTreesRegressor(),
    
    "LGBMRegressor":LGBMRegressor()
}

def BuiltModel(model):
    model.fit(x_train,y_train)
    y_pred = model.predict(x_train)
    pred = model.predict(x_test)

    r2score = r2_score(y_test,pred)*100

    #evaluation
    mse = mean_squared_error(y_test,pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test,pred)
    mape = mean_absolute_percentage_error(y_test,pred)
    print("MAE :", mae)
    print("RMSE :", rmse)
    print("MAPE :",mape)
    print('------------------------------')

    # r2 score
    print(f"Testing r2 Score:", r2score,"%")
    print('------------------------------')
    
    sns.set_style("whitegrid")
    sns.regplot(y_test,pred)
    plt.show()

for name, model in models.items():
  print("************************************************",name,"*****************************************************")
  BuiltModel(model)

model = RandomForestRegressor()
model.fit(x_train,y_train)
y_pred = model.predict(x_train)
pred = model.predict(x_test)

r2score = r2_score(y_test,pred)*100

 #evaluation
mse = mean_squared_error(y_test,pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test,pred)
mape = mean_absolute_percentage_error(y_test,pred)
print("MAE :", mae)
print("RMSE :", rmse)
print("MAPE :",mape)
print('------------------------------')

# r2 score
print(f"Testing r2 Score:", r2score,"%")
print('------------------------------')
    
sns.set_style("whitegrid")
sns.regplot(y_test,pred)
plt.show()

"""## Prediction on Test Data set"""

test.head()

p_ID = test['PropertyID']
test = test.drop(columns='PropertyID')

"""## DateOfEvaluation"""

test['Add_new'] = test['Address'].apply(lambda x : num_rem(x))

"""### Creating a new column using DateOfEvaluation &YearOfConstruction"""

test["Eval_year"] = test["DateOfEvaluation"].astype(str).apply(lambda x: x[0:4])

test["Eval_year"]=test["Eval_year"].astype('int64')
test['House_age'] = test["Eval_year"]-test['YearOfConstruction']
test["Eval_year"]=test["Eval_year"].astype('object')

test.describe().T

#dropping LandAreaInSqFt 
test.drop(columns=['LandAreaInSqFt'], inplace = True)

test.drop(columns=['NoOfCommercialUnits','TotalNoOfUnits','NoOfResidentialUnits'], inplace = True)

test= pd.get_dummies(test, columns=['Borough','TaxClass_AtEvaluationTime'],drop_first=True)

test['Lot'] = np.log(test['Lot'])
test['GrossAreaInSqFt'] = np.log(test['GrossAreaInSqFt'])

test.drop(columns=['DateOfEvaluation','Address'], inplace=True)

cat_data  = test.select_dtypes(include = 'object')
num_data = test.select_dtypes(include = [np.number])
from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
for i in cat_data.columns:
    cat_data[i] = enc.fit_transform(cat_data[i].values.reshape(-1,1))

num = num_data.reset_index(drop=True)
cat_data = cat_data.reset_index(drop=True)

"""### Combining categorical and numerical data"""

test = pd.concat([num, cat_data], axis = 1)

test.head()

"""## StandardScaler"""

#Lets bring all numerical features to common scale by applying standard scaler

scaler = StandardScaler()
test_std = scaler.fit_transform(test)
test_std = pd.DataFrame(test_std,columns=test.columns)

test_std.columns

test_std.shape

#lets predict the price with our best model
prediction = model.predict(test_std)

prediction

#lets make the dataframe for prediction
Prop_value = pd.DataFrame(prediction, columns=["PropertyEvaluationvalue"])

sub_file = pd.concat([p_ID, Prop_value], axis = 1)

sub_file

#Lets save the submission to csv
sub_file.to_csv("SNT.csv",index=False)

